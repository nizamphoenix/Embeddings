{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom tqdm import tqdm\ntqdm.pandas()\nimport os\n# Any results you write to the current directory are saved as output.\nimport string\nimport re    #for regex\nimport nltk\nfrom nltk.corpus import stopwords\nimport spacy\nfrom nltk import pos_tag\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize\n# Tweet tokenizer does not split at apostophes which is what we want\nfrom nltk.tokenize import TweetTokenizer   \nimport time\nimport re\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Credits: https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"## Load data\ntrain = pd.read_csv('/kaggle/input/google-quest-challenge/train.csv')\ntest = pd.read_csv('/kaggle/input/google-quest-challenge/test.csv')\nprint(\"Train shape : \",train.shape)\nprint(\"Test shape : \",test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pre-processing functions  \nvarious functions, viz removing special chars,punctuations,correcting spellings etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"def decontract(text):\n    \"\"\"\n    function to transform short-hand: can't--->can not\n    \"\"\"\n    text = re.sub(r\"(W|w)on(\\'|\\’)t \", \"will not \", text)\n    text = re.sub(r\"(C|c)an(\\'|\\’)t \", \"can not \", text)\n    text = re.sub(r\"(Y|y)(\\'|\\’)all \", \"you all \", text)\n    text = re.sub(r\"(Y|y)a(\\'|\\’)ll \", \"you all \", text)\n    text = re.sub(r\"(I|i)(\\'|\\’)m \", \"i am \", text)\n    text = re.sub(r\"(A|a)isn(\\'|\\’)t \", \"is not \", text)\n    text = re.sub(r\"n(\\'|\\’)t \", \" not \", text)\n    text = re.sub(r\"(\\'|\\’)re \", \" are \", text)\n    text = re.sub(r\"(\\'|\\’)d \", \" would \", text)\n    text = re.sub(r\"(\\'|\\’)ll \", \" will \", text)\n    text = re.sub(r\"(\\'|\\’)t \", \" not \", text)\n    text = re.sub(r\"(\\'|\\’)ve \", \" have \", text)\n    return text\n\ndef clean_text(x):\n    x = str(x)\n    for punct in \"/-'\":\n        x = x.replace(punct, ' ')\n    for punct in '&':\n        x = x.replace(punct, f' {punct} ')\n    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n        x = x.replace(punct, '')\n    return x\n\ndef clean_numbers(x):\n    x = re.sub('[0-9]{5,}', '12345', x)\n    x = re.sub('[0-9]{4}', '1234', x)\n    x = re.sub('[0-9]{3}', '123', x)\n    x = re.sub('[0-9]{2}', '12', x)\n    return x\n\nmispell_dict = {'colour':'color',\n                'centre':'center',\n                'didnt':'did not',\n                'doesnt':'does not',\n                'isnt':'is not',\n                'shouldnt':'should not',\n                'favourite':'favorite',\n                'travelling':'traveling',\n                'counselling':'counseling',\n                'theatre':'theater',\n                'canceled':'cancelled',\n                'labour':'labor',\n                'organisation':'organization',\n                'wwii':'world war 2',\n                'ww2':'world war 2',\n                'citicise':'criticize',\n                'instagram': 'social medium',\n                'whatsapp': 'social medium',\n                'snapchat': 'social medium',\n                'facebook': 'social medium',\n                'pinterest': 'social medium',\n                'linkedin': 'social medium'\n                }\ndef _get_mispell(mispell_dict):\n    \"\"\"\n    to rectify spellings as in identfier mispell_dict\n    \"\"\"\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\nmispellings, mispellings_re = _get_mispell(mispell_dict)\n\ndef replace_typical_misspell(text):\n    def replace(match):\n        return mispellings[match.group(0)]\n    return mispellings_re.sub(replace, text)\n\nuseless_punct = ['च', '不', 'ঢ়', '平', 'ᠠ', '錯', '判', '∙',\n                 '言', 'ς', 'ل', '្', 'ジ', 'あ', '得', '水', 'ь', '◦', '创', \n                 '康', '華', 'ḵ', '☺', '支', '就', '„', '」', '어', '谈', '陈', '团', '腻', '权', \n                 '年', '业', 'マ', 'य', 'ا', '売', '甲', '拼', '˂', 'ὤ', '贯', '亚', 'ि', '放', 'ʻ', 'ទ', 'ʖ', \n                 '點', '્', '発', '青', '能', '木', 'д', '微', '藤', '̃', '僕', '妒', '͜', 'ន', 'ध', '이', '希', '特',\n                 'ड', '¢', '滢', 'ส', '나', '女', 'క', '没', '什', 'з', '天', '南', 'ʿ', 'ค', 'も', '凰', '步', '籍', '西',\n                 'ำ', '−', 'л', 'ڤ', 'ៃ', '號', 'ص', 'स', '®', 'ʋ', '批', 'រ', '치', '谢', '生', '道', '═', '下', '俄', 'ɖ',\n                 '觀', 'வ', '—', 'ی', '您', '♥', '一', 'や', '⊆', 'ʌ', '語', 'ี', '兴', '惶', '瀛', '狐', '⁴', 'प', '臣', 'ద',\n                 '―', 'ì', 'ऌ', 'ీ', '自', '信', '健', '受', 'ɨ', '시', 'י', 'ছ', '嬛', '湾', '吃', 'ち', 'ड़', '反', '红', '有',\n                 '配', 'ে', 'ឯ', '宮', 'つ', 'μ', '記', '口', '℅ι', 'ो', '狸', '奇', 'о', 'ट', '聖', '蘭', '読', 'ū', '標', '要', \n                 'ត', '识', 'で', '汤', 'ま', 'ʀ', '局', 'リ', '्', 'ไ', '呢', '工', 'ल', '沒', 'τ', 'ិ', 'ö', 'せ', '你', 'ん', 'ュ', \n                 '枚', '部', '大', '罗', 'হ', 'て', '表', '报', '攻', 'ĺ', 'ฉ', '∩', '宝', '对', '字', '文', '这', '∑', '髪', 'り', '่', '능',\n                 '罢', '내', '阻', '为', '菲', 'ي', 'न', 'ί', 'ɦ', '開', '†', '茹', '做', '東', 'ত', 'に', 'ت', '晓', '키', '悲', 'સ', \n                 '好', '›', '上', '存', '없', '하', '知', 'ធ', '斯', ' ', '授', 'ł', '傳', '兰', '封', 'ோ', 'و', 'х', 'だ', '人', '太', \n                 '品', '毒', 'ᡳ', '血', '席', '剔', 'п', '蛋', '王', '那', '梦', 'ី', '彩', '甄', 'и', '柏', 'ਨ', '和', '坊', '⌚', '广', \n                 '依', '∫', 'į', '故', 'ś', 'ऊ', '几', '日', 'ک', '音', '×', '”', '▾', 'ʊ', 'ज', 'ด', 'ठ', 'उ', 'る', '清', 'ग', 'ط',\n                 'δ', 'ʏ', '官', '∛', '়', '้', '男', '骂', '复', '∂', 'ー', '过', 'য', '以', '短', '翻', 'র', '教', '儀', 'ɛ', '‹', 'へ', \n                 '¾', '合', '学', 'ٌ', '학', '挑', 'ष', '比', '体', 'م', 'س', 'អ', 'ת', '訓', '∀', '迎', 'វ', 'ɔ', '٨', '▒', '化', 'చ', '‛', \n                 'প', 'º', 'น', '업', '说', 'ご', '¸', '₹', '儿', '︠', '게', '骨', 'ท', 'ऋ', 'ホ', '茶', '는', 'જ', 'ุ', '羡', '節', 'ਮ', \n                 'উ', '番', 'ড়', '讲', 'ㅜ', '등', '伟', 'จ', '我', 'ล', 'す', 'い', 'ញ', '看', 'ċ', '∧', 'भ', 'ઘ', 'ั', 'ម', '街', 'ય', \n                 '还', '鰹', 'ខ', 'ు', '訊', 'म', 'ю', '復', '杨', 'ق', 'त', '金', '味', 'ব', '风', '意', '몇', '佬', '爾', '精', '¶', \n                 'ం', '乱', 'χ', '교', 'ה', '始', 'ᠰ', '了', '个', '克', '্', 'ห', '已', 'ʃ', 'わ', '新', '译', '︡', '本', 'ง', 'б', 'け', \n                 'ి', '明', '¯', '過', 'ك', 'ῥ', 'ف', 'ß', '서', '进', 'ដ', '样', '乐', '寧', '€', 'ณ', 'ル', '乡', '子', 'ﬁ', 'ج', '慕',\n                 '–', 'ᡵ', 'Ø', '͡', '제', 'Ω', 'ប', '絕', '눈', 'फ', 'ম', 'గ', '他', 'α', 'ξ', '§', 'ஜ', '黎', 'ね', '복', 'π', 'ú', '鸡',\n                 '话', '会', 'ক', '八', '之', '북', 'ن', '¦', '가', 'ו', '恋', '地', 'ῆ', '許', '产', 'ॡ', 'ش', '़', '野', 'ή', 'ɒ', '啧',\n                 'យ', '᠌', 'ᠨ', 'ب', '皎', '老', '公', '☆', 'व', 'ি', 'ល', 'ر', 'គ', '행', 'ង', 'ο', '让', 'ំ', 'λ', 'خ', 'ἰ', '家',\n                 'ট', 'ब', '理', '是', 'め', 'र', '√', '기', 'ν', '玉', '한', '入', 'ד', '别', 'د', 'ะ', '电', 'ા', '♫', 'ع', 'ં', '堵',\n                 '嫉', '伊', 'う', '千', '관', '篇', 'क', '非', '荣', '粵', '瑜', '英', '를', '美', '条', '`', '宋', '←', '수', '後', '•',\n                 '³', 'ी', '고', '肉', '℃', 'し', '漢', '싱', 'ϵ', '送', 'ه', '落', 'న', 'ក', 'க', 'ℇ', 'た', 'ះ', '中', '射', '♪', '符',\n                 'ឃ', '谷', '分', '酱', 'び', 'থ', 'ة', 'г', 'σ', 'と', '楚', '胡', '饭', 'み', '禮', '主', '直', '÷', '夢', 'ɾ', 'চ', '⃗',\n                 '統', '高', '顺', '据', 'ら', '頭', 'よ', '最', 'ా', 'ੁ', '亲', 'ស', '花', '≡', '眼', '病', '…', 'の', '發', 'ா', '汝',\n                 '★', '氏', 'ร', '景', 'ᡠ', '读', '件', '仲', 'শ', 'お', 'っ', 'پ', 'ᡤ', 'ч', '♭', '悠', 'ं', '六', '也', 'ռ', 'য়', '恐', \n                 'ह', '可', '啊', '莫', '书', '总', 'ষ', 'ք', '̂', '간', 'な', '此', '愛', 'ర', 'ใ', '陳', 'Ἀ', 'ण', '望', 'द', '请', '油',\n                 '露', '니', 'ş', '宗', 'ʍ', '鳳', 'अ', '邋', '的', 'ព', '火', 'ा', 'ก', '約', 'ட', '章', '長', '商', '台', '勢', 'さ',\n                 '국', 'Î', '簡', 'ई', '∈', 'ṭ', '經', '族', 'ु', '孫', '身', '坑', 'স', '么', 'ε', '失', '殺', 'ž', 'ર', 'が', '手',\n                 'ា', '心', 'ਾ', '로', '朝', '们', '黒', '欢', '早', '️', 'া', 'आ', 'ɸ', '常', '快', '民', 'ﷺ', 'ូ', '遢', 'η', '国', \n                 '无', '江', 'ॠ', '「', 'ন', '™', 'ើ', 'ζ', '紫', 'ె', 'я', '“', '♨', '國', 'े', 'อ', '∞', \n                  '\\n', \"{\\n', '}\\n\", \"=&gt;\", '}\\n\\n', '-&gt;', '\\n\\ni', '&lt;','/&gt;\\n','{\\n\\n','\\\\','|','&','\\\\n\\\\n',\"\\\\appendix\"]\nuseless_punct.remove(' ')\ndef remove_useless_punct(text):\n    \"\"\"\n    to remove punctuation symbols as in identifier useless_punct\n    \"\"\"\n    return re.sub(f'{\"|\".join(useless_punct)}', '', text)\n\nletter_mapping = {'\\u200b':' ', 'ũ': \"u\", 'ẽ': 'e', 'é': \"e\", 'á': \"a\", 'ķ': 'k', \n                  'ï': 'i', 'Ź': 'Z', 'Ż': 'Z', 'Š': 'S', 'Π': ' pi ', 'Ö': 'O', \n                  'É': 'E', 'Ñ': 'N', 'Ž': 'Z', 'ệ': 'e', '²': '2', 'Å': 'A', 'Ā': 'A',\n                  'ế': 'e', 'ễ': 'e', 'ộ': 'o', '⧼': '<', '⧽': '>', 'Ü': 'U', 'Δ': 'delta',\n                  'ợ': 'o', 'İ': 'I', 'Я': 'R', 'О': 'O', 'Č': 'C', 'П': 'pi', 'В': 'B', 'Φ': \n                  'phi', 'ỵ': 'y', 'օ': 'o', 'Ľ': 'L', 'ả': 'a', 'Γ': 'theta', 'Ó': 'O', 'Í': 'I',\n                  'ấ': 'a', 'ụ': 'u', 'Ō': 'O', 'Ο': 'O', 'Σ': 'sigma', 'Â': 'A', 'Ã': 'A', 'ᗯ': 'w', \n                  'ᕼ': \"h\", \"ᗩ\": \"a\", \"ᖇ\": \"r\", \"ᗯ\": \"w\", \"O\": \"o\", \"ᗰ\": \"m\", \"ᑎ\": \"n\", \"ᐯ\": \"v\", \"н\": \n                  \"h\", \"м\": \"m\", \"o\": \"o\", \"т\": \"t\", \"в\": \"b\", \"υ\": \"u\",  \"ι\": \"i\",\"н\": \"h\", \"č\": \"c\", \"š\":\n                  \"s\", \"ḥ\": \"h\", \"ā\": \"a\", \"ī\": \"i\", \"à\": \"a\", \"ý\": \"y\", \"ò\": \"o\", \"è\": \"e\", \"ù\": \"u\", \"â\": \n                  \"a\", \"ğ\": \"g\", \"ó\": \"o\", \"ê\": \"e\", \"ạ\": \"a\", \"ü\": \"u\", \"ä\": \"a\", \"í\": \"i\", \"ō\": \"o\", \"ñ\": \"n\",\n                  \"ç\": \"c\", \"ã\": \"a\", \"ć\": \"c\", \"ô\": \"o\", \"с\": \"c\", \"ě\": \"e\", \"æ\": \"ae\", \"î\": \"i\", \"ő\": \"o\", \"å\": \n                  \"a\", \"Ä\": \"A\",\"&gt\":\" greater than\",\"&lt\" :\"lesser than\", \"(not\" : \"not\" , \"});\":\"\",\">\" :\"greater\",\"<\":\"lesser\" ,\"$\":\"dollar\",\"\\\\\\\\\":\" \",\"\\\\\": \" \"} \ndef clean_special_chars(text):\n    \"\"\"\n    clean weird / special characters as in identifier letter_mapping\n    \"\"\"\n    new_text = ''\n    for i in range(len(text)):\n        if i in letter_mapping:\n            c = letter_mapping[i]\n        else:\n            c = text[i]\n        new_text += c\n    return new_text\n\ndef clean_apostrophes(sentence):\n    apostrophes = [\"’\", \"‘\", \"´\", \"`\"]\n    for s in apostrophes:\n        sentence = re.sub(s, \"'\", sentence)\n    return sentence\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\ntqdm.pandas()\ndef build_vocab(sentences, verbose =  True):\n    \"\"\"\n     this function constructs vocubulary from sentences of a corpus\n    :param sentences: list of list of words\n    :return: dictionary of words and their count\n    \"\"\"\n    vocab = {}\n    for sentence in tqdm(sentences, disable = (not verbose)):\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nimport operator \ndef check_coverage(vocab,embeddings_index):\n    \"\"\"\n    function to check coverage between vocabulary and the word embedding\n    :param vocab: dictionary of wordtypes from the corpus\n    :param embeddings_index: embeddings\n    :return: dictionary, consisting of out of vocabulary words w.r.t embeddings passed as argument\n    \"\"\"\n    word_embedding = {}\n    oov = {}\n    k = 0\n    i = 0\n    for word in tqdm(vocab):#vocab has unique words in a corpus:wordtypes/vocabulary of a corpus\n        try:\n            word_embedding[word] = embeddings_index[word]\n            k += vocab[word]#frequency of word in the corpus\n        except:\n            oov[word] = vocab[word]\n            i += vocab[word]\n            pass\n    print('Found embeddings for {:.2%} of vocabulary'.format(len(word_embedding) / len(vocab)))\n    print('Found embeddings for {:.2%} of corpus'.format(k / (k + i)))\n    sorted_x = sorted(oov.items(), key=operator.itemgetter(1),reverse=True)\n    return sorted_x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building vocabulary"},{"metadata":{"trusted":true},"cell_type":"code","source":"colname=\"question_body\"\nsentences = train[colname].progress_apply(lambda x: x.split()).values\nvocab = build_vocab(sentences)\nprint(\"vocabulary size for\",colname,\":\",len(vocab))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The following 4-steps are used for any of the pre-processing techniques above, \n1.Apply the **pre-processing** method of choice on the **sentence**.  \n2.Split the **sentence**.  \n3.**build** the vocabulary.  \n4.check **coverage**."},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(embeddings_index):\n    #preprocess method: decontract(sentence)\n    train[colname] = train[colname].progress_apply(lambda sentence: decontract(sentence))\n    sentences = train[colname].apply(lambda x: x.split())\n    vocab = build_vocab(sentences)\n    oov = check_coverage(vocab,embeddings_index)\n    #preprocess method:clean_apostrophes(sentence)\n    train[colname] = train[colname].progress_apply(lambda sentence: clean_apostrophes(sentence))\n    sentences = train[colname].apply(lambda x: x.split())\n    vocab = build_vocab(sentences)\n    oov = check_coverage(vocab,embeddings_index)\n    #preprocess method:clean_special_chars(sentence)\n    train[colname] = train[colname].progress_apply(lambda sentence: clean_special_chars(sentence))\n    sentences = train[colname].apply(lambda x: x.split())\n    vocab = build_vocab(sentences)\n    oov = check_coverage(vocab,embeddings_index)\n    #preprocess method: remove_useless_punct(sentence)\n    train[colname] = train[colname].progress_apply(lambda sentence: remove_useless_punct(sentence))\n    sentences = train[colname].apply(lambda x: x.split())\n    vocab = build_vocab(sentences)\n    oov = check_coverage(vocab,embeddings_index)\n    #preprocess method:clean_text(sentence)\n    train[colname] = train[colname].progress_apply(lambda sentence: clean_text(sentence))\n    sentences = train[colname].apply(lambda x: x.split())\n    vocab = build_vocab(sentences)\n    oov = check_coverage(vocab,embeddings_index)\n    #preprocess method:clean_numbers(sentence)\n    train[colname] = train[colname].progress_apply(lambda sentence: clean_numbers(sentence))\n    sentences = train[colname].apply(lambda x: x.split())\n    vocab = build_vocab(sentences)\n    oov = check_coverage(vocab,embeddings_index)\n    #preprocess method:replace_typical_misspell(sentence)\n    train[colname] = train[colname].progress_apply(lambda sentence: replace_typical_misspell(sentence))\n    sentences = train[colname].progress_apply(lambda x: x.split())\n    to_remove = ['a','to','of','and']\n    sentences = [[word for word in sentence if not word in to_remove] for sentence in tqdm(sentences)]\n    vocab = build_vocab(sentences)\n    oov = check_coverage(vocab,embeddings_index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let us now use GloVe embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\ndef load_embeddings(path):\n    with open(path,'rb') as f:\n        emb_arr = pickle.load(f)\n    return emb_arr\nGLOVE_EMBEDDING_PATH = '/kaggle/input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl' \ntic = time.time()\nglove_embeddings = load_embeddings(GLOVE_EMBEDDING_PATH)\nprint(f'loaded {len(glove_embeddings)} word vectors in {time.time()-tic}s')\n#check coverage with glove embedding\noov = check_coverage(vocab,glove_embeddings)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/google-quest-challenge/train.csv')\ntest = pd.read_csv('/kaggle/input/google-quest-challenge/test.csv')\ntrain['host'].apply(in set(train['host']).difference(set(test['host']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/google-quest-challenge/train.csv')\ntest = pd.read_csv('/kaggle/input/google-quest-challenge/test.csv')\nto_del_indices = train[(train['host']=='meta.christianity.stackexchange.com')|(train['host']=='rpg.stackexchange.com')].index\nset(Counter(train['host']).keys()).difference(set(Counter(test['host']).keys()))\n# Delete these row indexes from dataFrame\nprint(\"before deletion, train.shape:\", train.shape)\ntrain.drop(to_del_indices , inplace=True)\nprint(\"after deletion, train.shape:\", train.shape)\n\nprint(\"Test shape : \",test.shape)\npreprocess(glove_embeddings)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/google-quest-challenge/train.csv')\ntest = pd.read_csv('/kaggle/input/google-quest-challenge/test.csv')\nto_del_indices = train[(train['host']=='meta.christianity.stackexchange.com')|(train['host']=='rpg.stackexchange.com')].index\n\n# Delete these row indexes from dataFrame\nprint(\"before deletion, train.shape:\", train.shape)\ntrain.drop(to_del_indices , inplace=True)\nprint(\"after deletion, train.shape:\", train.shape)\n\nprint(\"Test shape : \",test.shape)\ncolname=\"answer\"\npreprocess(glove_embeddings)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Found GloVe embeddings for 49.51% of \"question_body\" vocabulary  \nFound GloVe embeddings for 92.11% of \"question_body\" corpus  \nFound GloVe embeddings for 52.33% of \"answer\" vocabulary  \nFound GloVe embeddings for 94.36% of \"answer\" corpus  "},{"metadata":{},"cell_type":"markdown","source":" ## Loading fasttext crawl"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \nfrom gensim.models import KeyedVectors\nnews_path = '/kaggle/input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\nembeddings_index = KeyedVectors.load_word2vec_format(news_path, binary=False)\n#checking coverage with fasttext embedding\noov = check_coverage(vocab,embeddings_index)#without preprocessing vocabulary","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading fasttext wikitext"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\ndef load_embeddings(path):\n    with open(path,'rb') as f:\n        emb_arr = pickle.load(f)\n    return emb_arr\nFASTTEXT_EMBEDDING_PATH = '/kaggle/input/fasttext-wikinews/wiki-news-300d-1M.pickle' \ntic = time.time()\nfasttext_embeddings = load_embeddings(FASTTEXT_EMBEDDING_PATH)\nprint(f'loaded {len(glove_embeddings)} word vectors in {time.time()-tic}s')\n#check coverage with glove embedding\noov = check_coverage(vocab,fasttext_embeddings)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/google-quest-challenge/train.csv')\ntest = pd.read_csv('/kaggle/input/google-quest-challenge/test.csv')\nto_del_indices = train[(train['host']=='meta.christianity.stackexchange.com')|(train['host']=='rpg.stackexchange.com')].index\n\n# Delete these row indexes from dataFrame\nprint(\"before deletion, train.shape:\", train.shape)\ntrain.drop(to_del_indices , inplace=True)\nprint(\"after deletion, train.shape:\", train.shape)\n6\nprint(\"Test shape : \",test.shape)\npreprocess(fasttext_embeddings)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport transformers\nimport torch.nn as nn\nimport pandas as pd\nimport numpy as np\nfrom sklearn import model_selection\nfrom transformers import AdamW,get_linear_schedule_with_warmup\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multioprocessing as xmp\nimport torch_xla.distributed.parallel_loader as pl\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BERTBaseUncased(nn.Module):\n    def __init__(self,bert_path):\n        super(BERTBaseUncased,self).__init__()\n        self.bert_path=bert_path\n        self.bert=transformers.BertModel.from_pretrained(self_path)\n        self.bert_drop=nn.Dropout(0.3)\n        self.out=nn.Linear(768,30)\n    def forward(self,ids,mask,token_type_ids):\n        _,o2=self.bert(ids,attention_mask=mask,token_type_ids=token_type_ids)\n        bo=self.bert_drop(o2)\n        return self.out(bo)\n    \nclass BERTDatasetTraining:\n    def __init__(self,qtitle,qbody,answer,targets,tokenizer,max_len):\n        self.qtitle=qtitle\n        self.qbody=qbody\n        self.answer=answer\n        self.tokenizer=tokenizer\n        self.max_len=max_len\n        self.targets=targets\n    def __len__(self):\n        return len(self.answer)\n    def __getitem__(self,item):\n        question_title=str(self.qtitle[item])\n        question_body=str(self.qbody[item])\n        answer=str(self.answer[item])\n        inputs=self.tokenizer.encode_plus(\n            question_title+\" \"+question_body,\n            answer,\n            add_special_tokens=True,\n            max_length=self.max_len\n        )\n        ids=inputs[\"input_ids\"]\n        token_type_ids=inputs[\"token_type_ids\"]\n        mask=inputs[\"attention_mask\"]\n        padding_len=self.max_len-len(ids)\n        ids=ids+([0]*padding_len)\n        token_type_ids=token_type_ids+([0]*padding_len)\n        mask=mask+([0]*padding_len)\n        return{\n            \"ids\":torch.tensor(ids,dtype=torch.long),\n                \"mask\":torch.tensor(mask,dtype=torch.long),\n              \"token_type_ids\":torch.tensor(token_type_ids,dtype=torch.long),\n               \"targets\":torch.tensor(self.targets[item,:],dtype=torch.float)\n        }\n    \n    def loss_fn(outputs,targets):\n        return nn.BCEWithLogitsLoss()(outputs,targets)\n    \n    def train_loop_fn(data_loader,model,optimizer,device,scheduler=None):\n        model.train()\n        for bi,d in enumerate(data_loader):\n            ids=d[\"ids\"]\n            mask=d[\"mask\"]\n            token_type_ids=d[\"token_type_ids\"]\n            targets=d[\"targets\"]\n            ids=ids.to(device,dtype=torch.long)\n            mask=mask.to(device,dtype=torch.long)\n            token_type_ids=token_type_ids.to(device,dtype=torch.long)\n            targets=targets.to(device,dtype=torch.float)\n            \n            optimizer.zero_grad()\n            outputs=model(ids=ids,mask=mask,token_type_ids=token_type_ids)\n            loss=loss_fn(outputs,targets)\n            loss.backward()\n            xm.optimizer_step(optimizer)\n            if scheduler is not None:\n                scheduler.step()\n            if bi%10==0:\n                xm.master_print(f\"bi={bi},loss={loss}\")\n            \n    def eval_loop_fn(data_loader,model,device):\n        model.eval()\n        fin_targets=[]\n        fin_outputs=[]\n        for bi,d in enumerate(data_loader):\n            ids=d[\"ids\"]\n            mask=d[\"mask\"]\n            token_type_ids=d[\"token_type_ids\"]\n            targets=d[\"targets\"]\n            ids=ids.to(device,dtype=torch.long)\n            mask=mask.to(device,dtype=torch.long)\n            token_type_ids=token_type_ids.to(device,dtype=torch.long)\n            targets=targets.to(device,dtype=torch.float)\n        \n            outputs=model(ids=ids,mask=mask,token_type_ids=token_type_ids)\n            loss=loss_fn(outputs,targets)\n            fin_targets.append(targets.cpu().detach().numpy())\n            fin_outputs.append(outputs.cpu().detach().numpy())\n        return np.vstack(fin_outputs),np.vtack(fin_targets)\n    \n    def run(index):\n        MAX_LEN=512\n        TRAIN_BATCH_SIZE=32\n        EPOCHS=20\n        dfx=pd.read_csv(\"train.csv\").fillna(\"none\")\n        df_train,df_valid=model_selection.train_test_split(dfx,random_state=42,test_size=0.1)\n        df_train = df_train.reset_index(drop=True)\n        df_valid = df_valid.reset_index(drop=True)\n        \n        sample=pd.read_csv(\"submission.csv\")\n        target_cols=list(sample.drop(\"qa_id\",axis=1).columns)\n        train_targets = df_train[target_cols].values\n        valid_targets = df_valid[target_cols].values\n        \n        tokenizer=transformers.BertTokenizer.from_pretrained(\"bert_base_uncased\")\n        \n        train_dataset=BERTDatasetTraining(\n        qtitle=df_train.question_title.values,\n        qbody=df_train.question_body.values,\n        answer=df_train.answer.values,\n        targets=train_targets,\n        tokenizer=tokenizer,\n        max_len=MAX_LEN\n        )\n        train_sampler = torch.utils.data.DistributedSampler(\n            train_dataset,\n            num_replicas=xm.xrt_world_size(),\n            rank=xm.get_ordinal(),\n            shuffle=True\n        )\n        train_data_loader=torch.utils.data.DataLoader(\n            train_dataset,\n            batch_size=TRAIN_BATCH_SIZE,\n            sampler=train_sampler\n        )\n        \n        valid_dataset=BERTDatasetTraining(\n        qtitle=df_valid.question_title.values,\n        qbody=df_valid.question_body.values,\n        answer=df_valid.answer.values,\n        targets=valid_targets,\n        tokenizer=tokenizer,\n        max_len=MAX_LEN\n        )\n        valid_sampler = torch.utils.data.DistributedSampler(\n            valid_dataset,\n            num_replicas=xm.xrt_world_size(),\n            rank=xm.get_ordinal(),\n        )\n        valid_data_loader=torch.utils.data.DataLoader(\n            valid_dataset,\n            batch_size=4,###change\n            sampler=valid_sampler\n        )\n        \n        device=xm.xla_device()\n        lr=3e-5*xm.xrt_world_size()###change\n        num_train_steps=int(len(train_dataset)/TRAIN_BATCH_SIZE/xm.xrt_world_size()*EPOCHS)\n        model=BERTBaseUncased(\"bert_base_uncased\")\n        \n        optimizer=AdamW(model.parametes(),lr=lr)\n        scheduler=get_linear_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=0,\n            num_training_steps=num_train_steps\n        )\n        \n        for epoch in range(EPOCHS):\n            para_loader=pl.ParallelLoader(train_data_loader,[device])\n            train_loop_fn(para_loader.per_device_loader(device),model,optimizer,device,scheduler)\n            para_loader=pl.ParallelLoader(valid_data_loader,[device])\n            o,t=eval_loop_fn(para_loader.per_device_loader(device),model,device)\n            \n            spear=[]\n            for jj in range(t.shape[1]):\n                p1=list(t[:,jj])\n                p2=list(o[:,jj])\n                coef,_=np.nan_to_num(stats.spearmanr(p1,p2))\n                spear.append(coef)\n            spear=np.mean(spear)\n            xm.master_print(f\"epoch={epoch},spearman={spear}\")\n            xm.save(model.state_dict(),\"model.bin\")\nif __name__==\"__main__\":\n    xmp.spawn(run,nprocs=8)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}